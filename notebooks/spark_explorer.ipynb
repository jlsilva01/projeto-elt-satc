{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaração das bibliotecas/pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "id": "EHdeeUNGSRUb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "from delta import *\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"py4j\").setLevel(logging.DEBUG)\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos dados do arquivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o6a6T_sRSnKX"
   },
   "outputs": [],
   "source": [
    "# Carrega variáveis do .env\n",
    "dotenv_path = os.path.join(os.path.dirname(\"..\"), \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "account_name = os.getenv(\"ADLS_ACCOUNT_NAME\")\n",
    "landing_container = os.getenv(\"ADLS_FILE_SYSTEM_NAME\")\n",
    "bronze_container = os.getenv(\"ADLS_BRONZE_CONTAINER\")\n",
    "client_id = os.getenv(\"ADLS_SP_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"ADLS_SP_CLIENT_SECRET\")\n",
    "tenant_id = os.getenv(\"ADLS_SP_TENANT_ID\")\n",
    "\n",
    "if not all([account_name, landing_container, bronze_container, client_id, client_secret, tenant_id]):\n",
    "    raise ValueError(\"Variáveis de ambiente não carregadas corretamente. Verifique o .env.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validação do carregamento correto das variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze\n"
     ]
    }
   ],
   "source": [
    "print (bronze_container);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do Spark Session com os pacotes para o Delta Lake e Azure ADLS, usando credencial por Service Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.account.oauth2.client.endpoint.datalakefd251f50081bbe5a.dfs.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: fs.azure.account.oauth2.client.secret.datalakefd251f50081bbe5a.dfs.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: fs.azure.account.oauth2.client.id.datalakefd251f50081bbe5a.dfs.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: fs.azure.account.auth.type.datalakefd251f50081bbe5a.dfs.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: fs.azure.account.oauth.provider.type.datalakefd251f50081bbe5a.dfs.core.windows.net\n",
      "25/06/18 16:53:40 WARN Utils: Your hostname, NOTEDELL3420 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/06/18 16:53:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/mnt/c/codigos/projeto-elt-satc-airflow/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jlsilva01/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jlsilva01/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-azure added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9a048c75-8893-4ce3-9e85-790ef09f5d1b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.4 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.2.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 606ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.2.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   26  |   0   |   0   |   0   ||   26  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9a048c75-8893-4ce3-9e85-790ef09f5d1b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 26 already retrieved (0kB/12ms)\n",
      "25/06/18 16:53:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Criação da SparkSession com autenticação via OAuth 2.0\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"LandingToBronze\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-azure:3.3.4\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.azurebfs.impl\", \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\")\n",
    "    .config(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "    .config(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    .config(f\"fs.azure.account.oauth2.client.id.{account_name}.dfs.core.windows.net\", client_id)\n",
    "    .config(f\"fs.azure.account.oauth2.client.secret.{account_name}.dfs.core.windows.net\", client_secret)\n",
    "    .config(f\"fs.azure.account.oauth2.client.endpoint.{account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Reflete as configurações no Hadoop Configuration também (essencial para `FileSystem.get()`)\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "hadoop_conf.set(f\"fs.azure.account.oauth.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "hadoop_conf.set(f\"fs.azure.account.oauth2.client.id.{account_name}.dfs.core.windows.net\", client_id)\n",
    "hadoop_conf.set(f\"fs.azure.account.oauth2.client.secret.{account_name}.dfs.core.windows.net\", client_secret)\n",
    "hadoop_conf.set(f\"fs.azure.account.oauth2.client.endpoint.{account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação do seção do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "dLVRL7feT-cX",
    "outputId": "f9dc706f-a3a5-4f6b-8073-79fbc053a5fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LandingToBronze</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f92399cf170>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de variáveis de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define os caminhos ABFSS\n",
    "subdir = os.getenv(\"ADLS_DIRECTORY_NAME\", \"\")\n",
    "\n",
    "landing_path = f\"abfss://{landing_container}@{account_name}.dfs.core.windows.net/{subdir}\"\n",
    "bronze_path = f\"abfss://{bronze_container}@{account_name}.dfs.core.windows.net/{subdir}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checagem do correto carregamento das variaveis de ambiente setadas no bloco anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4mjoy7AUAfW",
    "outputId": "283707f5-ecc4-4bb6-fed8-ae7ea1e0b060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfss://landing-zone@datalakefd251f50081bbe5a.dfs.core.windows.net/sistema_seguro/carro.csv\n"
     ]
    }
   ],
   "source": [
    "path = f\"{landing_path}/carro.csv\"\n",
    "\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura de um arquivo CSV do container landing-zone no ADLS Gen2 da Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddrmZAIXUMcD",
    "outputId": "41d4d93e-c075-48cf-9149-e6e66ff22fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+----------+----+--------+\n",
      "|  placa|   modelo|     chassi|     marca| ano|     cor|\n",
      "+-------+---------+-----------+----------+----+--------+\n",
      "|ALD3834|     CLIO|34574215969|   RENAULT|2011|  BRANCO|\n",
      "|CCR8096|    CRETA|88547875547|   HYUNDAI|2020|  BRANCO|\n",
      "|DLA3438|    PUNTO|98823483434|      FIAT|2013|   PRETO|\n",
      "|EEE1056|ECO SPORT|56753453455|      FORD|2020|    AZUL|\n",
      "|FFR1234|    PALIO|32383478747|      FIAT|2009| AMARELO|\n",
      "|GQY6753|      S10|72004160549|        GM|2015|   PRETO|\n",
      "|IAC8974|   TIGUAN|77130757746|VOLKSWAGEN|2022|    AZUL|\n",
      "|JIE0952|   PASSAT|87493270405|VOLKSWAGEN|2016|   CINZA|\n",
      "|JNU7898|     2020|87628347687|      FORD|2020|   VERDE|\n",
      "|LVX7086|  SANDERO|00025131958|   RENAULT|1999|VERMELHO|\n",
      "|LWJ9156|     ONIX|40991078801|        GM|2015|    AZUL|\n",
      "|MZT1826|      GOL|41150439528|VOLKSWAGEN|1998| AMARELO|\n",
      "|NAP5760|  COMPASS|40364369549|      JEEP|2017|   PRETO|\n",
      "|NEM5116|     2008|69469771523|   PEUGEOT|2018|   PRETO|\n",
      "|NFT2212|     KWID|12344343433|   RENAULT|2023|  BRANCO|\n",
      "|XYZ1234|     ONIX|12312312312|        GM|2002| AMARELO|\n",
      "+-------+---------+-----------+----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"{landing_path}/carro.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura de um arquivo tipo Delta Lake do container bronze no ADLS Gen2 da Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/18 16:58:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+----------+----+--------+--------------------+\n",
      "|  placa|   modelo|     chassi|     marca| ano|     cor|_ingestion_timestamp|\n",
      "+-------+---------+-----------+----------+----+--------+--------------------+\n",
      "|ALD3834|     CLIO|34574215969|   RENAULT|2011|  BRANCO|2025-06-18 16:00:...|\n",
      "|CCR8096|    CRETA|88547875547|   HYUNDAI|2020|  BRANCO|2025-06-18 16:00:...|\n",
      "|DLA3438|    PUNTO|98823483434|      FIAT|2013|   PRETO|2025-06-18 16:00:...|\n",
      "|EEE1056|ECO SPORT|56753453455|      FORD|2020|    AZUL|2025-06-18 16:00:...|\n",
      "|FFR1234|    PALIO|32383478747|      FIAT|2009| AMARELO|2025-06-18 16:00:...|\n",
      "|GQY6753|      S10|72004160549|        GM|2015|   PRETO|2025-06-18 16:00:...|\n",
      "|IAC8974|   TIGUAN|77130757746|VOLKSWAGEN|2022|    AZUL|2025-06-18 16:00:...|\n",
      "|JIE0952|   PASSAT|87493270405|VOLKSWAGEN|2016|   CINZA|2025-06-18 16:00:...|\n",
      "|JNU7898|     2020|87628347687|      FORD|2020|   VERDE|2025-06-18 16:00:...|\n",
      "|LVX7086|  SANDERO|00025131958|   RENAULT|1999|VERMELHO|2025-06-18 16:00:...|\n",
      "|LWJ9156|     ONIX|40991078801|        GM|2015|    AZUL|2025-06-18 16:00:...|\n",
      "|MZT1826|      GOL|41150439528|VOLKSWAGEN|1998| AMARELO|2025-06-18 16:00:...|\n",
      "|NAP5760|  COMPASS|40364369549|      JEEP|2017|   PRETO|2025-06-18 16:00:...|\n",
      "|NEM5116|     2008|69469771523|   PEUGEOT|2018|   PRETO|2025-06-18 16:00:...|\n",
      "|NFT2212|     KWID|12344343433|   RENAULT|2023|  BRANCO|2025-06-18 16:00:...|\n",
      "|XYZ1234|     ONIX|12312312312|        GM|2002| AMARELO|2025-06-18 16:00:...|\n",
      "+-------+---------+-----------+----------+----+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(f\"{bronze_path}/carro\")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
